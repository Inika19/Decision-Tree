{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:17.465929Z",
     "start_time": "2020-02-13T18:15:15.155872Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading iris dataset into dataframe\n",
    "\n",
    "removing Id column as that isnt a feature. Changing column names to improve readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:18.276315Z",
     "start_time": "2020-02-13T18:15:18.249020Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "df.drop('Id', axis = 1, inplace= True)\n",
    "cols = list(df.columns)\n",
    "df = df.rename(columns={cols[0]: \"Sepal Length\", cols[1]: \"Sepal Width\", cols[2]: \"Petal Length\", cols[3]: \"Petal Width\", cols[4]: \"Label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:19.176422Z",
     "start_time": "2020-02-13T18:15:19.123170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal Length  Sepal Width  Petal Length  Petal Width        Label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:22.405447Z",
     "start_time": "2020-02-13T18:15:22.386502Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.values # a 2D numpy array corresponding to the dataset (without column headers)\n",
    "targets = np.unique(data[:,-1]) # a list of all output classes or targets or labels for iris flowers in the dataset\n",
    "features = list(df.columns) # a list of all features in the dataset. Also contains 'Label', which is not a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T19:35:15.653625Z",
     "start_time": "2020-01-12T19:35:15.639131Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = data[:,0:4]\n",
    "# y = data[:,-1]\n",
    "\n",
    "# print(\"data shapes:\",x.shape, y.shape)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x,y, random_state = 0)\n",
    "# print(\"train data shapes:\",x_train.shape, y_train.shape,\"\\ntest data shapes\",x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to implement the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:25.118128Z",
     "start_time": "2020-02-13T18:15:25.103271Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    ''' Is used to check if the data belonging to a node is pure, i.e. if all flowers contained in the node are of \n",
    "    the same type. If the node is pure, it returns true otherwise false.'''\n",
    "    label_column = data[:, -1] # np array containing labels for each data point\n",
    "    unique_classes = np.unique(label_column) # np array containing all unique labels in the node\n",
    "    #same as np.array(list(set(label_column)))\n",
    "\n",
    "    if len(unique_classes) == 1: #pure node ie leaf reached\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:26.895614Z",
     "start_time": "2020-02-13T18:15:26.878336Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    ''' Returns the output class for the node by majority vote.'''\n",
    "    label_column = data[:, -1] # np array containing labels for each data point\n",
    "    unique_classes, class_counts = np.unique(label_column, return_counts=True) #returns 2 lists. \n",
    "    #unique classes contains all unique labels contained in the node \n",
    "    #class_counts is an array containing the number of times each unique label appears in the data,\n",
    "    #in the same order as in unique_classes\n",
    "\n",
    "    index = class_counts.argmax() #index of element having highest value in class_counts\n",
    "    output_class = unique_classes[index] #contains label corresponding to index ie. output class that appears most \n",
    "    #frequently in the data\n",
    "    \n",
    "    return output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:27.327215Z",
     "start_time": "2020-02-13T18:15:27.312589Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "    '''Returns a dictionary poetntial_splits, containing index of the feature/column as the key with a list as\n",
    "    its value, that contains all potential splits for that feature.'''\n",
    "    potential_splits = {} #creates empty dictionary\n",
    "    _, n_columns = data.shape #n_columns corresponds to number of columns in the data\n",
    "    for column_index in range(n_columns - 1):   # excluding the last column as it is the label\n",
    "        potential_splits[column_index] = []     # creating empty list as value for column index key\n",
    "        values = data[:, column_index]          # values contains all values in the current column in the data\n",
    "        unique_values = np.unique(values)       # all unique values are found among them\n",
    "\n",
    "        for index in range(1,len(unique_values)): \n",
    "            current_value = unique_values[index]\n",
    "            previous_value = unique_values[index - 1]\n",
    "            potential_split = (current_value + previous_value) / 2    #potential split is the mid point\n",
    "                                                                      #between every 2 consecutive (unique) values\n",
    "            potential_splits[column_index].append(potential_split) #append potential split found to value list \n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:27.857040Z",
     "start_time": "2020-02-13T18:15:27.848466Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    '''Splits the data passed to it about the split value of the split column.'''\n",
    "    split_column_values = data[:, split_column]        # array containing all values of the split_column in the data\n",
    "    data_split = np.delete(data,split_column,axis = 1) #removing split_column from the data as that feature is not \n",
    "                                                       #to be used again to split\n",
    "    data_below = data_split[split_column_values <= split_value] #all datapoints having value of split_column <=\n",
    "                                                                #the split_value\n",
    "    data_above = data_split[split_column_values >  split_value] #all datapoints having value of split_column > \n",
    "                                                                #the supplied split_value\n",
    "    \n",
    "    return data_below, data_above                               #split data is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:28.368894Z",
     "start_time": "2020-02-13T18:15:28.351334Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    '''Used to calculate entropy of the data.'''\n",
    "    label_column = data[:, -1]                              #labels in the data\n",
    "    _, counts = np.unique(label_column, return_counts=True) #only counts returned by unique are used\n",
    "\n",
    "    probabilities = counts / counts.sum()                   #as numpy arrays perform element-wise operations, \n",
    "                                                            #probabilities are found as count of the label/total count of all labels\n",
    "    entropy = sum(probabilities * -np.log2(probabilities)) #formula for entropy summed over all unqiue classes\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:28.964971Z",
     "start_time": "2020-02-13T18:15:28.947236Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    '''Used to find entropy after a split is performed.'''\n",
    "    n = len(data_below) + len(data_above)                            #total number of data points\n",
    "    p_data_below = len(data_below) / n                               #probabilities are found for splits\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below)  #weighted addition of required information or \n",
    "                      + p_data_above * calculate_entropy(data_above)) #entropies\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:29.393521Z",
     "start_time": "2020-02-13T18:15:29.383086Z"
    }
   },
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits):\n",
    "    '''Is used to determine the best possible split (having highest information gain) out of all potential splits.'''\n",
    "    overall_entropy = 9999                           #an arbitrary high value\n",
    "    for column_index in potential_splits:            #iterate over column indices in potential_splits\n",
    "        for value in potential_splits[column_index]: #iterate over all potential splits for that column\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value) #perform split\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above) #find entropy after split\n",
    "            if current_overall_entropy <= overall_entropy: #split resulting in lowest entropy after split is chosen\n",
    "                overall_entropy = current_overall_entropy  #as that will lead to max information gain\n",
    "                best_split_column = column_index           #and thus will be the best possible split\n",
    "                best_split_value = value\n",
    "    #best split column, value and resulting entropy is returned\n",
    "    return best_split_column, best_split_value, overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:29.897333Z",
     "start_time": "2020-02-13T18:15:29.880164Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_decision_tree(data, features, counter=0):\n",
    "    '''Recursive function to construct decision tree. The tree is a dictionary containing questions\n",
    "    (corresponding to the split that is decided) as keys with key values being a list of 2 elements corresponding to \n",
    "    if the answer to the question is yes and no respectively.'''\n",
    "    # base cases: if data is pure or all features have been used up to make splits\n",
    "    if (check_purity(data)) or (len(features)==0):\n",
    "        classification = classify_data(data)\n",
    "        return classification #majority vote \n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1\n",
    "        # calling helper functions \n",
    "        potential_splits = get_potential_splits(data) # a dictionary of potential splits\n",
    "        split_column, split_value, _ = determine_best_split(data, potential_splits) #_ because overall entropy is not used \n",
    "        feature_name = features[split_column] #feature name is found using column index \n",
    "        features.remove(feature_name) #split feature is removed from list of features so no more splits are made on\n",
    "                                      #the same feature\n",
    "        data_below, data_above = split_data(data, split_column, split_value) #data is split according to the\n",
    "                                                                             #best split that was found \n",
    "        \n",
    "       \n",
    "        # instantiate sub-tree\n",
    "        question = \"{} <= {}\".format(feature_name, split_value) #split recorded in sub tree as a question\n",
    "        sub_tree = {question: []} #empty list created as value for the key that is the question\n",
    "        \n",
    "        # find answers (recursion)\n",
    "        yes_answer = build_decision_tree(data_below, features, counter) #recursive call for 'yes' answer of question\n",
    "        no_answer = build_decision_tree(data_above, features, counter) #recursive call for 'no' answer of question\n",
    "        \n",
    "        sub_tree[question].append(yes_answer) #append answers to list for question\n",
    "        sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:30.393914Z",
     "start_time": "2020-02-13T18:15:30.362323Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def display_output(tree, data, features, counter = 0):\n",
    "    '''Uses the tree created to print output in the required format.'''\n",
    "    counter += 1 #increment counter to keep track of levels\n",
    "    label_column = data[:,-1] #contains all labels in the data\n",
    "    targets, counts = np.unique(label_column, return_counts=True) # 2 lists containing all unique targets or labels\n",
    "                                                                  # and their counts respectively\n",
    "    print(\"Level\", counter-1)\n",
    "    for i in range(len(counts)):\n",
    "        print(\"Count of\", targets[i] ,\"=\",counts[i])\n",
    "    current_entropy = calculate_entropy(data)                     # to find current entropy\n",
    "    print(\"Current entropy is =\", current_entropy)\n",
    "    if current_entropy == 0:                                      # base case: leaf node is reached\n",
    "        print(\"Reached leaf node\\n\")\n",
    "    \n",
    "    elif len(features)==0:                                        # base case: when all features have been used up\n",
    "        print(\"No more features left to split upon\")\n",
    "        return\n",
    "    \n",
    "    else:                                                         # recursion will take place here\n",
    "        key = list(tree.keys())[0]                                # to find the key\n",
    "        question = key.split()                                    # list of words and operators in the question \n",
    "        feature_name = \" \".join(question[:2]) #feature name is obtained by joining first two words of the question\n",
    "        split_value = float(question[-1])     #split value is the last term in the question. it has to be converted \n",
    "                                              #to float to make comparisions later\n",
    "        print(\"Splitting on feature\", feature_name,\"with gain ratio \")\n",
    "        index = features.index(feature_name)  #to find index of splitting feature in features list\n",
    "        data_below, data_above = split_data(data, index, split_value) #perform split\n",
    "        split_entropy = calculate_overall_entropy(data_below, data_above) #find entropy\n",
    "        features.remove(feature_name) #remove feature that was used to make split from list of features\n",
    "        \n",
    "        info_gain = current_entropy - split_entropy #find information gain\n",
    "        r = counts / counts.sum()\n",
    "        split_info = sum(-r * np.log10(r)) #find split info\n",
    "        gain_ratio = info_gain/split_info  #find gain ratio\n",
    "        print(gain_ratio) \n",
    "        print()\n",
    "    \n",
    "        #recursive calls\n",
    "        left_child = tree[key][0] #find left child of current node in tree\n",
    "        display_output(left_child, data_below, features, counter) #call on left child\n",
    "        right_child = tree[key][1] #find right child of current node in tree\n",
    "        display_output(right_child, data_above, features, counter) #call on right child\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "The decision tree has been implemented as a dictionary with questions corresponding to the decided split as the key, having 2 values in a list. The first value corresponds to if the answer to the question is yes and the second to if the answer is no.   \n",
    "The answers may be a string, containing the output class, in case a pure or leaf node has been reached or if all features have been used up and no more splits can be performed.   \n",
    "Otherwise, the answer is a dictionary of the same format, corresponding to a subtree characterising further splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:31.750889Z",
     "start_time": "2020-02-13T18:15:31.654478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Petal Width <= 0.8': ['Iris-setosa', {'Petal Length <= 4.75': [{'Sepal Length <= 4.95': [{'Sepal Width <= 2.45': ['Iris-versicolor', 'Iris-virginica']}, 'Iris-versicolor']}, 'Iris-virginica']}]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = build_decision_tree(data,features[:4]) #features slice is passed so as to not pass the string 'Label' as\n",
    "print(tree,\"\\n\")                              #it is not a feature name. print the tree here. \n",
    "# pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "Here is the tree printed in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:15:52.824532Z",
     "start_time": "2020-02-13T18:15:52.798367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of Iris-setosa = 50\n",
      "Count of Iris-versicolor = 50\n",
      "Count of Iris-virginica = 50\n",
      "Current entropy is = 1.584962500721156\n",
      "Splitting on feature Petal Width with gain ratio \n",
      "1.924659245361106\n",
      "\n",
      "Level 1\n",
      "Count of Iris-setosa = 50\n",
      "Current entropy is = 0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level 1\n",
      "Count of Iris-versicolor = 50\n",
      "Count of Iris-virginica = 50\n",
      "Current entropy is = 1.0\n",
      "Splitting on feature Petal Length with gain ratio \n",
      "2.1837483292096787\n",
      "\n",
      "Level 2\n",
      "Count of Iris-versicolor = 44\n",
      "Count of Iris-virginica = 1\n",
      "Current entropy is = 0.15374218032876188\n",
      "Splitting on feature Sepal Length with gain ratio \n",
      "2.3616109695158807\n",
      "\n",
      "Level 3\n",
      "Count of Iris-versicolor = 1\n",
      "Count of Iris-virginica = 1\n",
      "Current entropy is = 1.0\n",
      "Splitting on feature Sepal Width with gain ratio \n",
      "3.321928094887362\n",
      "\n",
      "Level 4\n",
      "Count of Iris-versicolor = 1\n",
      "Current entropy is = 0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level 4\n",
      "Count of Iris-virginica = 1\n",
      "Current entropy is = 0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level 3\n",
      "Count of Iris-versicolor = 43\n",
      "Current entropy is = 0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level 2\n",
      "Count of Iris-versicolor = 6\n",
      "Count of Iris-virginica = 49\n",
      "Current entropy is = 0.4971677614160753\n",
      "No more features left to split upon\n"
     ]
    }
   ],
   "source": [
    "display_output(tree, df.values, features[:4]) #output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
